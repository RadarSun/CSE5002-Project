{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project\n",
    "# Part 3: Classification using adjacent dataset and attribute dataset\n",
    "Author: Zhicong Sun  \n",
    "SID: 12032471  \n",
    "Date: 2021.5.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Read dataset and assign feature names to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "feature names:  ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6'] \n\n\ndata.head:\n    feature1  feature2  feature3  feature4  feature5  feature6\n0         1         2         8        22         0     23574\n1         2         2        15        14       229         0\n2         1         1         3         0       237      9976\n3         2         2         3         0       230     19776\n4         1         2         6         0       229     17097 \n\ntrain data: \n    feature1  feature2  feature3  feature4  feature5  feature6\n0         1         2         8        22         0     23574\n1         2         2        15        14       229         0\n2         1         1         3         0       237      9976\n3         2         2         3         0       230     19776\n4         1         2         6         0       229     17097 \n\nshape of train data:  (4000, 6) \n\ntrain label:\n     label\n0   2009\n1   2004\n2   2008\n3   2002\n4   2007 \n\nshape of train label:  (4000, 1) \n\ntest data:\n     feature1  feature2  feature3  feature4  feature5  feature6\n0         2         1         7        23       268      2584\n1         1         2         8         0       235     50908\n2         1         2        16         0       261      3737\n3         5         1         6         0       280         0\n4         5         2         3         0       283      4481 \n\nshape of test data:  (1298, 6) \n\ntest label: \n    label\n0   2005\n1   2008\n2   2006\n3   2005\n4   2005 \n\nshape of test label:  (1298, 1) \n\n"
     ]
    }
   ],
   "source": [
    "# read data from txt file\n",
    "attr_data = pd.read_csv(\"attr.csv\",header = None,usecols = [1,2,3,4,5,6])\n",
    "train_label = pd.read_csv(\"label_train.csv\",header=None,usecols = [1])\n",
    "# assign a feature name to each column of data set, test set and label\n",
    "feature_names = []\n",
    "for i in range(attr_data.shape[1]):\n",
    "    feature_names.append(\"feature\" + str(i+1))\n",
    "attr_data = pd.read_csv(\"attr.csv\",header = None,usecols = [1,2,3,4,5,6],names=feature_names)\n",
    "train_data = pd.read_csv(\"attr.csv\",header = None,usecols = [1,2,3,4,5,6],nrows=4000,names=feature_names)\n",
    "train_label = pd.read_csv(\"label_train.csv\",header=None,usecols = [1],names=[\"label\"])\n",
    "test_data = pd.read_csv(\"attr.csv\",header = None,usecols = [1,2,3,4,5,6],skiprows = 4000,nrows=1298,names=feature_names)\n",
    "test_label = pd.read_csv(\"label_test.csv\",header=None,usecols = [1],names=[\"label\"])\n",
    "\n",
    "\n",
    "print(\"feature names: \",feature_names,\"\\n\")\n",
    "print(\"\\ndata.head:\\n\",attr_data.head(),\"\\n\")\n",
    "print(\"train data: \\n\",train_data.head(),\"\\n\")\n",
    "print(\"shape of train data: \",train_data.shape,\"\\n\")\n",
    "print(\"train label:\\n \",train_label.head(),\"\\n\")\n",
    "print(\"shape of train label: \",train_label.shape,\"\\n\")\n",
    "print(\"test data:\\n \",test_data.head(),\"\\n\")\n",
    "print(\"shape of test data: \",test_data.shape,\"\\n\")\n",
    "print(\"test label: \\n\",test_label.head(),\"\\n\")\n",
    "print(\"shape of test label: \",test_label.shape,\"\\n\")\n"
   ]
  },
  {
   "source": [
    "## 2 Read the  adjacency matrix and obtain accessible matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "attr_data.head:\n    feature1  feature2  feature3  feature4  feature5  feature6\n0         1         2         8        22         0     23574\n1         2         2        15        14       229         0\n2         1         1         3         0       237      9976\n3         2         2         3         0       230     19776\n4         1         2         6         0       229     17097 \n\nShape of attr_data:  (5298, 6) \n\ntrain_label.head:\n    label\n0   2009\n1   2004\n2   2008\n3   2002\n4   2007 \n\nShape of train_label:  (4000, 1)\nadj_matrix:\n       0     1     2     3     4     5     6     7     8     9     ...  5288  \\\n0        0     0     0     0     0     0     0     0     0     0  ...     0   \n1        0     0     0     0     0     1     0     0     0     0  ...     0   \n2        0     0     0     0     0     0     0     0     0     0  ...     0   \n3        0     0     0     0     0     0     0     0     0     0  ...     0   \n4        0     0     0     0     0     0     0     0     0     0  ...     0   \n...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n5293     0     0     0     0     0     0     0     0     0     0  ...     0   \n5294     0     0     0     0     0     0     0     0     0     0  ...     0   \n5295     0     0     0     0     0     0     0     0     0     0  ...     0   \n5296     0     0     0     0     0     0     0     0     0     0  ...     0   \n5297     0     0     0     0     0     0     0     0     0     0  ...     0   \n\n      5289  5290  5291  5292  5293  5294  5295  5296  5297  \n0        0     0     0     0     0     0     0     0     0  \n1        1     0     0     0     0     0     0     0     0  \n2        0     0     0     0     0     0     0     0     0  \n3        0     0     0     0     0     0     0     0     0  \n4        0     0     0     0     0     0     0     0     0  \n...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n5293     0     0     0     0     0     0     0     0     0  \n5294     0     0     0     0     0     0     0     0     0  \n5295     0     0     0     0     0     0     0     0     0  \n5296     0     0     0     0     0     0     0     0     0  \n5297     0     0     0     0     0     0     0     0     0  \n\n[5298 rows x 5298 columns]\n"
     ]
    }
   ],
   "source": [
    "# get the adj matrix\n",
    "tep_adjdata = []\n",
    "with open('adjlist.csv','r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        tep_adjdata.append(row)\n",
    "adjlist_data = []\n",
    "for i in range(len(tep_adjdata)):\n",
    "    if len(tep_adjdata[i]) == 2 and tep_adjdata[i][1] == '':\n",
    "        tep_adjdata[i].pop()\n",
    "    tmp = []\n",
    "    for j in range(len(tep_adjdata[i])):\n",
    "        tmp.append(int(tep_adjdata[i][j]))\n",
    "    adjlist_data.append(tmp)\n",
    "adjmatrix = np.zeros((5298,5298),dtype = int)\n",
    "for row in range(len(adjlist_data)):\n",
    "    for column in range(len(adjlist_data[row])):\n",
    "        if column != 0:\n",
    "            adjmatrix[row][adjlist_data[row][column]] = 1\n",
    "adjmatrix = pd.DataFrame(adjmatrix)\n",
    "\n",
    "\n",
    "print(\"attr_data.head:\\n\",attr_data.head(),\"\\n\")\n",
    "print(\"Shape of attr_data: \",attr_data.shape,\"\\n\")\n",
    "print(\"train_label.head:\\n\",train_label.head(),\"\\n\")\n",
    "print(\"Shape of train_label: \",train_label.shape) \n",
    "print(\"adj_matrix:\\n\",adjmatrix)"
   ]
  },
  {
   "source": [
    "## 3 Divide features into categorical value and continous value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------------------------\nfeature1 : [1 2 5 3 6 4]\n---------------------------\nfeature2 : [2 1 0]\n---------------------------\nfeature3 : [ 8 15  3  6  7 10  0 14  2 18 16  1  9 13 12 29 39  5 30 27  4 17 33 24\n 25 31 44 22 11 28 34 32 26 41 20 43 21 23 37 35 42 19 40]\n---------------------------\nfeature4 : [22 14  0 10  6  7 15 24 28  9 30  3 25 23 18 29  4 16 36 41 26  8 13  5\n 17 12  1  2 31 27 34 11 19 33 37 21 43 35 20 39 44 32 40]\n---------------------------\nfeature5 : [  0 229 237 230 286 268 283 236 227 228 274 275 256 231 255 258 278 241\n 251 261 240 233 276 254 246 265 247 232 285 244 264 253 242 272 250 234\n 270 257 260 259 245 249 267 248 235 262 280 284 266 243 263 288 282 269\n 271 252 239 279 290 281 287 289 273 238]\n---------------------------\nfeature6 : [23574     0  9976 ... 53700  9884 53767]\n\ncategorical features: ['feature1', 'feature2']\n\ncontinous_val features: ['feature3', 'feature4', 'feature5', 'feature6']\n"
     ]
    }
   ],
   "source": [
    "# divide features into categorical value and continous value\n",
    "categorical_val = []\n",
    "continous_val = []\n",
    "for column in train_data.columns:\n",
    "    print('---------------------------')\n",
    "    print(f\"{column} : {train_data[column].unique()}\")\n",
    "    if len(train_data[column].unique()) <= 10:\n",
    "        categorical_val.append(column)\n",
    "    else:\n",
    "        continous_val.append(column)\n",
    "print(\"\\ncategorical features:\",categorical_val)\n",
    "print(\"\\ncontinous_val features:\",continous_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Dummy coding of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(5298, 13)\n(4000, 13)\n(1298, 13)\n"
     ]
    }
   ],
   "source": [
    "# dummy coding of categorical features\n",
    "total_data = pd.concat([train_data,test_data],axis = 0)\n",
    "total_dataset = pd.get_dummies(total_data, columns = categorical_val)\n",
    "print(total_dataset.shape)\n",
    "train_dataset = total_dataset[0:4000][:]\n",
    "print(train_dataset.shape)\n",
    "test_dataset = total_dataset[4000:][:]\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Features scaling of continous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4000, 5311)\n(1298, 5311)\n"
     ]
    }
   ],
   "source": [
    "# standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas import Series\n",
    "standard_scaler = StandardScaler()\n",
    "data_aft_stdsscsl = train_dataset\n",
    "data_aft_stdsscsl[continous_val] = standard_scaler.fit_transform(train_dataset[continous_val])\n",
    "data_aft_stdsscsl.head()\n",
    "data_aft_stdsscsl = pd.concat([data_aft_stdsscsl,adjmatrix[0:4000]], axis=1)\n",
    "data_aft_stdsscsl.head()\n",
    "print(data_aft_stdsscsl.shape)\n",
    "\n",
    "test_data_aft_stdsscsl = test_dataset\n",
    "test_data_aft_stdsscsl[continous_val] = standard_scaler.fit_transform(test_dataset[continous_val])\n",
    "tmp_adjmatrix = adjmatrix[4000:]\n",
    "adjmatrix_column_id = [i for i in range(1298)]\n",
    "tmp_adjmatrix.index = Series(adjmatrix_column_id)\n",
    "test_data_aft_stdsscsl.head()\n",
    "test_data_aft_stdsscsl = pd.concat([test_data_aft_stdsscsl,tmp_adjmatrix], axis=1)\n",
    "test_data_aft_stdsscsl.head()\n",
    "print(test_data_aft_stdsscsl.shape)"
   ]
  },
  {
   "source": [
    "## 6 Neural network classifier and K-fold cross validation "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fold: 0 ------------\n",
      "Epoch: 0 | train accuracy: 0.00\n",
      "Epoch: 2 | train accuracy: 0.80\n",
      "Epoch: 4 | train accuracy: 0.92\n",
      "Epoch: 6 | train accuracy: 0.95\n",
      "0.7923538230884558\n",
      "fold: 1 ------------\n",
      "Epoch: 0 | train accuracy: 0.02\n",
      "Epoch: 2 | train accuracy: 0.75\n",
      "Epoch: 4 | train accuracy: 0.92\n",
      "Epoch: 6 | train accuracy: 0.94\n",
      "0.8124531132783196\n",
      "fold: 2 ------------\n",
      "Epoch: 0 | train accuracy: 0.00\n",
      "Epoch: 2 | train accuracy: 0.72\n",
      "Epoch: 4 | train accuracy: 0.89\n",
      "Epoch: 6 | train accuracy: 0.92\n",
      "0.7561890472618155\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.utils.data as Data\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "\n",
    "# get the number of features, it is also the number of input neurons\n",
    "n_feature = data_aft_stdsscsl.shape[1]\n",
    "# process training data, tensor\n",
    "data_aft_stdsscsl = np.array(data_aft_stdsscsl)\n",
    "data_aft_stdsscsl = torch.tensor(data_aft_stdsscsl)\n",
    "data_aft_stdsscsl = data_aft_stdsscsl.to(torch.float32)\n",
    "# process test data, tensor\n",
    "test_data_aft_stdsscsl = np.array(test_data_aft_stdsscsl)\n",
    "test_data_aft_stdsscsl = torch.tensor(test_data_aft_stdsscsl)\n",
    "test_data_aft_stdsscsl = test_data_aft_stdsscsl.to(torch.float32)\n",
    "# process train labels and test labels at the same time\n",
    "train_label = np.array(train_label).flatten()\n",
    "test_label = np.array(test_label).flatten()\n",
    "le = LabelEncoder()\n",
    "total_label = np.hstack((train_label, test_label))\n",
    "le.fit(total_label)\n",
    "label_cnt = len(le.classes_)\n",
    "train_label = le.transform(train_label)\n",
    "train_label = torch.tensor(train_label)\n",
    "test_label = le.transform(test_label)\n",
    "test_label = torch.tensor(test_label)\n",
    "\n",
    "# K-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for fold,(train_id,valiation_id) in enumerate(skf.split(data_aft_stdsscsl,train_label)):\n",
    "    print(\"fold:\",fold,\"------------\")\n",
    "    kfold_train_data = data_aft_stdsscsl[train_id]\n",
    "    kfold_train_label = train_label[train_id]\n",
    "    kfold_test_data = data_aft_stdsscsl[valiation_id]\n",
    "    kfold_test_label = train_label[valiation_id]\n",
    "    net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(n_feature, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, label_cnt),\n",
    "    )\n",
    "\n",
    "    # optimizer is a tool used for training\n",
    "    optimizer  = torch.optim.Adam(net.parameters(), lr=0.01, betas=(0.9, 0.99))\n",
    "    # when calculating errors, it is noted that the real value is not one-hot, but 1D tensor (batch,)\n",
    "    # but the predicted value is 2D tensor (batch, n_ classes)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    torch.manual_seed(1)    # reproducible\n",
    "    BATCH_SIZE = 64      # number of batch training data\n",
    "    # first, convert it into a dataset recognized by torch\n",
    "    torch_dataset = Data.TensorDataset(kfold_train_data,kfold_train_label)\n",
    "    # put the dataset into the dataloader\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               # do you want to scramble the data,yes\n",
    "        num_workers=2,              # multithreading to read data\n",
    "    )\n",
    "\n",
    "    for epoch in range(8):\n",
    "        for step ,(batch_x,batch_y) in enumerate(loader):\n",
    "            out = net(batch_x)     # feed net training data x, output analysis value\n",
    "            loss = loss_func(out, batch_y)     # calculate the error between the two\n",
    "            optimizer.zero_grad()   # clear the residual update parameter value of the previous step\n",
    "            loss.backward()         # the error is back propagated and the updated values of parameters are calculated\n",
    "            optimizer.step()        # apply the parameter update value to the parameters of net\n",
    "            if epoch % 2 == 0 and step == 0:\n",
    "                prediction = torch.max(out, 1)[1]\n",
    "                pred_y = prediction.data.numpy()\n",
    "                target_y = batch_y.data.numpy()\n",
    "                accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)\n",
    "                print('Epoch:',epoch,'| train accuracy: %.2f' % accuracy)\n",
    "\n",
    "    torch.save(net.state_dict(), 'net_params.pkl')   # only save the parameters in the network (fast, less memory)\n",
    "\n",
    "    # build a new net for predicting\n",
    "    predict_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(n_feature, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, label_cnt),\n",
    "    )\n",
    "    # copy saved parameters to predict_net\n",
    "    predict_net.load_state_dict(torch.load('net_params.pkl'),False)\n",
    "    prediction = predict_net(kfold_test_data)\n",
    "    prediction = torch.max(prediction, 1)[1]\n",
    "    pred_y = prediction.data.numpy()\n",
    "    target_y = kfold_test_label.data.numpy()\n",
    "    accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)\n",
    "    print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Final training and prediction  \n",
    "***\n",
    "**Note:**\n",
    "Now all the train data can be used to train the ensembel model, and the trained model will be used to predict the test data.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0 | train accuracy: 0.00\n",
      "Epoch: 2 | train accuracy: 0.72\n",
      "Epoch: 4 | train accuracy: 0.89\n",
      "Epoch: 6 | train accuracy: 0.92\n",
      "predicted label:  [2005 2008 2006 ... 2007 2008 2005]\n",
      "Accuracy on test set:  80.12326656394453 %\n"
     ]
    }
   ],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_feature, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, label_cnt),\n",
    ")\n",
    "\n",
    "# optimizer is a tool used for training\n",
    "optimizer  = torch.optim.Adam(net.parameters(), lr=0.01, betas=(0.9, 0.99))\n",
    "# when calculating errors, it is noted that the real value is not one-hot, but 1D tensor (batch,)\n",
    "# but the predicted value is 2D tensor (batch, n_ classes)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "BATCH_SIZE = 64      # number of batch training data\n",
    "# first, convert it into a dataset recognized by torch\n",
    "torch_dataset = Data.TensorDataset(kfold_train_data,kfold_train_label)\n",
    "# put the dataset into the dataloader\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # do you want to scramble the data,yes\n",
    "    num_workers=2,              # multithreading to read data\n",
    ")\n",
    "\n",
    "for epoch in range(8):\n",
    "    for step ,(batch_x,batch_y) in enumerate(loader):\n",
    "        out = net(batch_x)     # feed net training data x, output analysis value\n",
    "        loss = loss_func(out, batch_y)     # calculate the error between the two\n",
    "        optimizer.zero_grad()   # clear the residual update parameter value of the previous step\n",
    "        loss.backward()         # the error is back propagated and the updated values of parameters are calculated\n",
    "        optimizer.step()        # apply the parameter update value to the parameters of net\n",
    "        if epoch % 2 == 0 and step == 0:\n",
    "            prediction = torch.max(out, 1)[1]\n",
    "            pred_y = prediction.data.numpy()\n",
    "            target_y = batch_y.data.numpy()\n",
    "            accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)\n",
    "            print('Epoch:',epoch,'| train accuracy: %.2f' % accuracy)\n",
    "\n",
    "torch.save(net.state_dict(), 'net_params.pkl')   # only save the parameters in the network (fast, less memory)\n",
    "\n",
    "# build a new net for predicting\n",
    "predict_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_feature, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, label_cnt),\n",
    ")\n",
    "# copy saved parameters to predict_net\n",
    "predict_net.load_state_dict(torch.load('final_net_params.pkl'),False)\n",
    "prediction = predict_net(test_data_aft_stdsscsl)\n",
    "prediction = torch.max(prediction, 1)[1]\n",
    "pred_y = prediction.data.numpy()\n",
    "target_y = test_label.data.numpy()\n",
    "real_pred_y = le.inverse_transform(pred_y)\n",
    "print(\"predicted label: \",real_pred_y)\n",
    "accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)\n",
    "print(\"Accuracy on test set: \",accuracy*100,\"%\")\n",
    "np.savetxt(\"predict_testlabel_with_adjlist.txt\",real_pred_y)"
   ]
  },
  {
   "source": [
    "## 8 Disscuss the performance of this classifier\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "micro_precision: 0.8012326656394453\nmacro_precision: 0.299221153063926\nweighted_precision: 0.7862371149684687\nmicro_f1_score: 0.8012326656394453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,f1_score\n",
    "micro_precision = precision_score(target_y, pred_y, average=\"micro\")\n",
    "macro_precision = precision_score(target_y, pred_y, average=\"macro\")\n",
    "weighted_precision = precision_score(target_y, pred_y, average=\"weighted\")\n",
    "micro_f1_score = f1_score(target_y, pred_y, average='micro')\n",
    "print(\"micro_precision:\", micro_precision)\n",
    "print(\"macro_precision:\", macro_precision)\n",
    "print(\"weighted_precision:\", weighted_precision)\n",
    "print(\"micro_f1_score:\", micro_f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}